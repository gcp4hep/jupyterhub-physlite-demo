{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a6712b-1df1-4818-b56b-36118d86a4f2",
   "metadata": {},
   "source": [
    "# Demo: `DAOD_PHYSLITE` analysis with uproot/awkward on jupyterhub on GCP\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note: This tutorial is targeted at users interested in R&D and technical details. Much of this is still in early development/prototyping.\n",
    "</div>\n",
    "\n",
    "The image that runs on jupyterhub and the dask workers is defined by the following Dockerfile:\n",
    "\n",
    "https://github.com/gcp4hep/analysis-cluster/blob/16fb374fe26948081cf3f3b02117d05366d96520/daskhub/docker/jupyter-physlite/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476ab60-f01e-4a0a-8ec6-6eb35c756357",
   "metadata": {},
   "source": [
    "## Read and process PHYSLITE using uproot/awkward\n",
    "\n",
    "First, let's start with some general notes on reading `DAOD_PHYSLITE`\n",
    "\n",
    "The PHYSLITE ROOT files currently follow a similar structure as regular ATLAS xAODs\n",
    "\n",
    "They containing several trees, where the one holding the actual data is called `CollectionTree`. The others contain various forms of Metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145cd4e-63a4-4145-a993-d424021c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46341d-40e5-43de-8a13-cceef30ca38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot.open(\"data/DAOD_PHYSLITE_21.2.108.0.art.pool.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e084a-42b4-45ba-99ad-411f4abc1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4cbbb4-6933-4c94-ae87-8a6ebcdbf561",
   "metadata": {},
   "source": [
    "### 1-D vectors\n",
    "* All branches are stored with the **highest split level**\n",
    "* In most cases data stored in branches called `Aux.<something>` or `AuxDyn.<something>`\n",
    "* Typically **vectors of fundamental types**, like e.g. pt/eta/phi of particle collections\n",
    "* **can be read into numpy arrays efficiently using uproot** since data stored as contiguous blocks  \n",
    "(except for the 10-byte vector headers whoose positions are known from ROOT's event offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264d7f5-1b56-44a2-a049-2f19f1d6f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"CollectionTree\"].show(\"/AnalysisElectronsAuxDyn.(pt|eta|phi)$/i\", name_width=30, interpretation_width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2d506-2475-4e7d-977c-34e5b9d08fbb",
   "metadata": {},
   "source": [
    "### ElementLinks\n",
    "\n",
    "The most relevant exception to this: `ElementLink` branches:\n",
    "\n",
    "* provide cross references into other collections\n",
    "* **often 2-dimensional** (`vector<vector<ElementLink<...>>>`)\n",
    "* data part (`ElementLink`) is serialized as a **structure of 2 32bit unsigned integers**:\n",
    "  * hash `m_persKey`, identifying the target collection\n",
    "  * index `m_persIndex` identifying the array-index of the corresponding particle in the target collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9dd67-6f45-4726-9009-9fa5e8f2d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"].typename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7c2dd-ffaf-4138-ba56-94185f7c4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in f.file.streamer_named(\"ElementLinkBase\").elements:\n",
    "    print(f\"{element.member('fName')}: {element.member('fTypeName')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c86472-36bd-4455-8244-cca9f33403b0",
   "metadata": {},
   "source": [
    "Uproot can read this, but the loop that deserializes the data is done in python and therefore slow.\n",
    "\n",
    "This is not relevant for this very small file, but becomes important for larger files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183d0b7-346f-4799-af89-1ce1a56ffe05",
   "metadata": {},
   "source": [
    "This can be handled by [AwkwardForth](https://doi.org/10.1051/epjconf/202125103002) which is however currently (November 2021) not yet integrated with uproot.\n",
    "\n",
    "For now we can use a custom function `branch_to_array` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb2a87-249d-4ec5-a7fc-850817cf0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.deserialization_hacks import branch_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ad108-1d1b-4cc9-bd19-171c29db3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be240c05-fa94-4a81-8faf-02472d3bb44b",
   "metadata": {},
   "source": [
    "One can actually see a significant improvement already for the small file with only 40 events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb1766-5acd-4176-b199-4b42d63aadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using standard uproot\n",
    "f.file.array_cache.clear()\n",
    "f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718bee3-0b34-44d4-afbe-48ae22c98b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using numba\n",
    "f.file.array_cache.clear()\n",
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a462d-3026-4879-a0c1-891b8a1a36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using awkward forth\n",
    "f.file.array_cache.clear()\n",
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"], use_forth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ba774-ad57-40d0-b39d-12c51a024461",
   "metadata": {},
   "source": [
    "## Integration with `coffea.nanoevents`\n",
    "\n",
    "The PHYSLITE schema and the corresponding behavior classes are still under development - [CoffeaTeam/coffea#540](https://github.com/CoffeaTeam/coffea/issues/540) tracks the progress of some TODO items.\n",
    "\n",
    "For more information on `NanoEvents` see the [NanoEvents tutorial](https://github.com/CoffeaTeam/coffea/blob/master/binder/nanoevents.ipynb) or [Nick Smith's presentation](https://youtu.be/udzkE6t4Mck) at the [pyHEP 2020](https://indico.cern.ch/event/882824).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>The Goal:</b>\n",
    "    <ul>\n",
    "        <li>Work with object-oriented event data models, but stick to the array-at-a-time processing paradigm.<br> â†’ Struct/Object of arrays instead of Array of structs/objects</li>\n",
    "        <li>Hide the details from the user</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124644bb-ac41-43c2-9a5f-fb9a0088d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "\n",
    "# patch nanoevents to use the custom branch_to_array function\n",
    "from physlite_experiments.deserialization_hacks import patch_nanoevents\n",
    "patch_nanoevents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa024f-1b43-473d-847a-6982854d847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = NanoEventsFactory.from_root(\n",
    "    \"data/DAOD_PHYSLITE_21.2.108.0.art.pool.root\",\n",
    "    \"CollectionTree\",\n",
    "    schemaclass=PHYSLITESchema\n",
    ")\n",
    "events = factory.events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711d5ea-82b9-4801-bcba-f83a51c0e9e7",
   "metadata": {},
   "source": [
    "This groups particles and the available properties conveniently under one central `event` array\n",
    "\n",
    "* everything is lazy loading\n",
    "* cross referencing via ElementLinks already implemented for some collections\n",
    "* particles behave as LorentzVectors (can add them, calculate invariant masses and much more)\n",
    "\n",
    "See [my tutorial at the IRIS-HEP AGC tools workshop 2021](https://github.com/nikoladze/agc-tools-workshop-2021-physlite) for more technical details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b42c6-7dd5-4ef8-9027-8e9aecbd6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cdf33-0d49-43f7-a244-1376884150f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aebe36-ab05-41df-87b6-f4d78f44be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dbf93-e788-43af-89f4-fea097e3194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.trackParticles.z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698fb19-d255-459a-85ba-5eb7dd8a88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons[events.Electrons.pt > 10000].trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a6ce0-d89b-4253-ba11-a6a979ad941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f96869-ac2e-4c16-b3a0-04efef9ac73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children.pdgId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916948a-312c-48de-b08d-51bf480e4c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children.pdgId.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0fc00-b3d1-453f-8263-6dde5184587c",
   "metadata": {},
   "source": [
    "## Read data via HTTPS from google cloud storage (authentication via rucio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe6332-069c-439e-9ac5-1539f0d8fb83",
   "metadata": {},
   "source": [
    "We will use the following functions to authenticate to rucio and get signed urls on google cloud storage (GCS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f8229-3053-451c-bbea-c1c8db34271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import setup_rucio_and_proxy, get_signed_url, get_signed_url_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f8f03-276b-4583-8d02-db7263681a6a",
   "metadata": {},
   "source": [
    "For that we have to provide a VOMS proxy. To avoid the need for having the grid certificate and the voms tools on this jupyterhub instance we create the voms proxy outside (some machine where we have the voms tools and our grid certificate) and upload it to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b42925-4ea8-4a7f-b3a5-d96e58d9c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload\n",
    "upload = FileUpload()\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251296a2-c9b5-450f-9b98-2f871af1967e",
   "metadata": {},
   "source": [
    "Also, fill in your CERN account name here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b723d5-094e-4814-afd5-5947208db7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUCIO_ACCOUNT=\"nihartma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624494a-16d1-4d6f-9f4e-136aceaeca66",
   "metadata": {},
   "source": [
    "Then we setup the nescessary environment variables (fill in your cern account name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaecc2d-774c-48b4-a36b-d1a2a6e12844",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_rucio_and_proxy(upload.data[-1], rucio_account=RUCIO_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e38f1-88f4-4cc5-9994-0b87212ad6b4",
   "metadata": {},
   "source": [
    "Now we should be able to query rucio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbe046-c3e4-4f6f-bc20-b880ef5ca7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rucio.client\n",
    "rucio_client = rucio.client.Client(ca_cert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9e554-0bda-40e3-beab-9387bb53905d",
   "metadata": {},
   "source": [
    "*Note: we should probably install the CERN CA files into the container in the future such that we don't need to run with `ca_cert=False`*\n",
    "\n",
    "Let's get a list of all files in one data period, corresponding to around 10% of the whole Run2 data - around 10TB in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c56a1d-ccaf-4f12-af91-a50aadbc34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(rucio_client.list_files(\"data17_13TeV\", \"data17_13TeV.periodK.physics_Main.PhysCont.DAOD_PHYSLITE.grp17_v01_p4309\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b637740-e1bf-4fca-b115-8b35c5aab437",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b178-fc6c-465c-878a-9aed678d4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(file[\"bytes\"] for file in files) / 1024 ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a75ef1-22b0-4469-a530-61374cee93eb",
   "metadata": {},
   "source": [
    "The full Run2 dataset is replicated to GCS. To access it via https we can ask rucio for a signed url. Uproot can directly deal with http(s) urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353ee01-b541-49f3-98b3-69ce57542e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_signed_url(rucio_client, files[0][\"scope\"], files[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ade9f-7030-43c7-b3dc-6c9c345cbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_remote = uproot.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc588b-e751-485d-975f-e1456a041017",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_remote[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6b9a4-a030-4359-abee-25a77ad5cadd",
   "metadata": {},
   "source": [
    "Some notes on this:\n",
    "\n",
    "* GCS does not support multi-range requests (equivalent to xrootd vector reads), single-range requests are allowed\n",
    "* Single-range requests with the uproot `MultithreadedHTTPSource` are suboptimal\n",
    "* GCS seems fine with a huge number of parallel requests - this can be done with asyncio\n",
    "* However, oftentimes downloading the whole file is still faster async reading of partial chunks (but needs lot's of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27d263-4406-48bc-a717-0b7114e9cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(url):\n",
    "    return requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617004cb-6b0b-43ce-9959-71d314094512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b98122-3bff-4eb5-843e-71d07ec26354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "uproot.open(io.BytesIO(data))[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5279025-93c9-4278-9326-d704122d9ba6",
   "metadata": {},
   "source": [
    "I have an experimental implementation for an asyncio HTTPSource for uproot (should probably make a PR for uproot at some point or consider using an interface to fsspec which has a `cat_ranges` method that might be used for this).\n",
    "\n",
    "GCS seems fine with 100 parallel tcp connections (even for each worker on a larger cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53b818-357a-4ffc-9ec5-816011776b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.io import AIOHTTPSource\n",
    "\n",
    "class AIOHTTP100Source(AIOHTTPSource):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, tcp_connection_limit=100, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690236d-ff68-4fea-82bd-621e8e5c4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.open(url, http_handler=AIOHTTP100Source)[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77635c1e-613e-454d-95ab-8bb93aa63d0b",
   "metadata": {},
   "source": [
    "## Run an actual analysis with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba88f8e-5927-46de-8af7-6f568b3ee1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    factory = NanoEventsFactory.from_root(url, \"CollectionTree\", schemaclass=PHYSLITESchema, uproot_options=dict(http_handler=AIOHTTP100Source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc9d21-a71b-4333-b6a4-2e0f9aa1031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = factory.events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bc18a-6c36-42c8-9f15-babd8948c2f7",
   "metadata": {},
   "source": [
    "We are going to use the example from my [vCHEP presention](https://doi.org/10.1051/epjconf/202125103001) that tries to reproduce some \"[SUSYTools](https://gitlab.cern.ch/atlas/athena/tree/master/PhysicsAnalysis/SUSYPhys/SUSYTools)-like\" object selections and overlap removal for electrons, muons and jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378781f2-3661-482c-b905-6c1407b924f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.analysis_example import get_obj_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8de4c-99d8-44ee-be17-3aafa6794dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_decorated = get_obj_sel(events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2e6ac-87c5-4c57-9a33-ec4a9e21b298",
   "metadata": {},
   "source": [
    "This will create the fields `passOR`, `signal`, `baseline` to indicate which objects pass these selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec50240-5715-446c-bb48-e0c5cc837673",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_decorated.Electrons.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3576361-8a5b-45d4-b049-5078a98860c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_decorated.Jets.passOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb01b8-12ae-4a53-8c72-ae1443d6dbdc",
   "metadata": {},
   "source": [
    "Delta-R between electrons and jets without overlap removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b031aae-1f7b-4e87-a462-713fce438f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exx, jxx = ak.unzip(ak.cartesian([events_decorated.Jets, events_decorated.Electrons]))\n",
    "plt.hist(ak.flatten(exx.delta_r(jxx)).to_numpy(), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a54b96-5320-4b6d-8a84-fa8bb2e67b82",
   "metadata": {},
   "source": [
    "With overlap removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1036c-7ae5-4426-954c-44060a13075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "electrons_pass = events_decorated.Electrons[events_decorated.Electrons.passOR]\n",
    "jets_pass = events_decorated.Jets[events_decorated.Jets.passOR]\n",
    "exx, jxx = ak.unzip(ak.cartesian([electrons_pass, jets_pass]))\n",
    "plt.hist(ak.flatten(exx.delta_r(jxx)).to_numpy(), bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c933d-1753-4ecf-bc71-697ef0b0246a",
   "metadata": {},
   "source": [
    "For this simple demonstration, let's just count the number of objects passing the criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3ff64-ba7a-4d3e-939f-dd71cb42d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(events):\n",
    "    events = get_obj_sel(events)\n",
    "    return  {\n",
    "        collection: {\n",
    "            flag : ak.count_nonzero(events[collection][flag])\n",
    "            for flag in [\"baseline\", \"passOR\", \"signal\"]\n",
    "        } for collection in [\"Electrons\", \"Muons\", \"Jets\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d762f05-18d1-4d86-9c3a-d3a7c70b1194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(results):\n",
    "    out = {\n",
    "        collection: {\n",
    "            flag: 0\n",
    "            for flag in [\"baseline\", \"passOR\", \"signal\"]\n",
    "        } for collection in [\"Electrons\", \"Muons\", \"Jets\"]\n",
    "    }\n",
    "    for result in results:\n",
    "        for collection, flags in result.items():\n",
    "            for flag, count in flags.items():\n",
    "                out[collection][flag] += count\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ca48d2-a973-47e1-bc1e-a7bdad459b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_analysis(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dd451-cd84-4566-bda9-8fc8c1ce9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc40a49-473c-407e-8b57-e5d3eff4341c",
   "metadata": {},
   "source": [
    "## Run on a dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd05797-9ab2-4dd3-86fa-2e186723c4e3",
   "metadata": {},
   "source": [
    "Now we want to run this function in parallel on a large number of files. This jupyterhub instance also features a [dask-gateway](https://gateway.dask.org/) that allows us as a user to dynamically request a cluster on GCP nodes.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "* All python modules need to be installed in the container running on the dask workers\n",
    "* We don't have a shared filesystem on GCP -> all data has to be accessed from GCS (google cloud storage)\n",
    "\n",
    "Also see Fernandos instructions on https://github.com/gcp4hep/analysis-cluster/wiki/Daskhub-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb22f8-6302-4174-b9d2-39d6cfae079c",
   "metadata": {},
   "source": [
    "One can either create the cluster here, or in another notebook or terminal. We will choose the latter option and create and manage it in the notebook [`manage_cluster.ipynb`](manage_cluster.ipynb) and then connect here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9915e-c40f-47fa-b98b-ec1bad0049f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "clusters = gateway.list_clusters()\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553facf3-fd41-4461-8a36-aea4922364dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.connect(clusters[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1d6dd-9d4e-4230-834f-2331e34cc74c",
   "metadata": {},
   "source": [
    "Drag & Drop the Dashboard url from the folloing cell to the dask plugin window on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26397a44-bf63-43b4-92bc-282efd20b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384d123-8dc2-4647-aa00-71f9cc397e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3232b3-5259-4411-87cc-0e71b5761179",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9596681-ff66-42a8-bf7f-a893ac3486f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c34a4b-c3e8-4921-a3ef-560cf31636e9",
   "metadata": {},
   "source": [
    "The `utils.py` module is not in the docker container - therefore we need to make sure the workers have it. One way to do this is the `UploadFile` worker plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86536b41-bce0-4575-88eb-b0f5b3ca2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.diagnostics.plugin import UploadFile\n",
    "\n",
    "client.register_worker_plugin(UploadFile(\"utils.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c49ae-44f9-46de-85d0-73a5e691b2f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Unfortunately i currently see lot's of memory issues running in parallel with <code>coffea.nanoevents</code>, so for this part we use an old prototype of this where this seems to be less drastic.\n",
    "</div>\n",
    "\n",
    "One can also use [`coffea.processor`](https://github.com/CoffeaTeam/coffea/blob/master/binder/processor.ipynb) to run in parallel, but also this requires a bit of work to get it running with all the custom things we have in this notebook (namely the uproot source, the signed url retrieving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe6636-0e74-4874-8fac-59dca2c6822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.physlite_events import physlite_events\n",
    "from physlite_experiments.utils import subdivide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae67ab-4e96-4f45-8625-9b011cabe57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def run_old(url, max_chunksize=100000):\n",
    "    array_cache = {}\n",
    "    with uproot.open(\n",
    "        url, http_handler=AIOHTTP100Source, array_cache=array_cache\n",
    "    ) as f:\n",
    "        tree = f[\"CollectionTree\"]\n",
    "        entry_start = 0\n",
    "        results = []\n",
    "        n = tree.num_entries\n",
    "        for chunksize in subdivide(n, math.ceil(n / max_chunksize)):\n",
    "            entry_stop = entry_start + chunksize\n",
    "            events = physlite_events(\n",
    "                tree, entry_start=entry_start, entry_stop=entry_stop\n",
    "            )\n",
    "            entry_start = entry_stop\n",
    "            results.append(run_analysis(events))\n",
    "            array_cache.clear()\n",
    "    return merge(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70532afd-ba20-4e57-8c8f-efdc843f14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_old(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c9a43-b5f3-4cd1-b2da-d1d6ba99ee10",
   "metadata": {},
   "source": [
    "We have to extract the x509 proxy data to be able to serialize it to a function that is used on the workers. One could use the `UploadFile` worker plugin here alternatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb396d5a-3acc-4bb0-bf97-9a43d9cde7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x509_data = upload.data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee23c36-acc4-498f-95b5-fba0072f9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job(fileinfo):\n",
    "    url = get_signed_url_worker(\n",
    "        x509_data, fileinfo[\"scope\"], fileinfo[\"name\"], rucio_account=RUCIO_ACCOUNT, ca_cert=False\n",
    "    )\n",
    "    return run_old(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599b10f-c77c-498d-b586-921e6edf2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "job(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070e9d3-8a85-4f23-b0ed-714e8042c9a2",
   "metadata": {},
   "source": [
    "We're using the `futures` api of dask which is rather low level but gives us the most control for these R&D studies. It mimics python's [`concurrent.futures`](https://docs.python.org/3/library/concurrent.futures.html) module.\n",
    "\n",
    "The basic principle is: You submit a function and it's input arguments and get back a `Future` object (immediately):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3beac-0de4-4db0-ae98-c1cdebe5d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(job, files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf12599-9e59-4f96-8e89-2fb584a46cb1",
   "metadata": {},
   "source": [
    "When you call `.result()` it will block until the job is finished. One can chain multiple futures to create execution graphs. There is also [`dask.delayed`](https://docs.dask.org/en/stable/delayed.html) which is capable to abstractly create these graphs and exectute them on demand (potentially optimizing). Finally, there are dask collections for distributed equivalents of pandas DataFrames or numpy arrays and there is [one for awkward array in development](https://dask-awkward.readthedocs.io/en/latest/index.html). This will allow a higher level interface for all the things we are doing here in the future ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2279e83-82ca-469a-952a-6584a5423eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8f205-6d1a-433e-b980-f35b58bd54b0",
   "metadata": {},
   "source": [
    "Now, let's scale the cluster to a larger number of workers (e.g. 128) and run on a subset of files, corresponding to roughly 1TB of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947824d-2d77-4c6d-a787-a17b3ba17e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = files[::10]\n",
    "sum(info[\"bytes\"] for info in subset) / 1024 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041ee9b-4218-41ba-92ce-65c87fb7d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(job, subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df294dd1-f0ea-406d-b573-5eb6f2ab46a2",
   "metadata": {},
   "source": [
    "The following call will gather all results and block until all jobs are finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b70bf1-e919-491e-af53-401d0093a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c95e9-f9ad-4965-b0b9-4587425e9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f59c85-3ccc-4435-b5cf-53aab92757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = merge(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34985df4-d7fb-4972-86b5-e6f29724c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30351160-c6fc-4215-8a5b-57cd9e9b2514",
   "metadata": {},
   "source": [
    "## Don't forget to shutdown the cluster afterwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd00dbc-711f-4c5e-b778-0ffd4cf4bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
