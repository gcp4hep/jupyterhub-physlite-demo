{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a6712b-1df1-4818-b56b-36118d86a4f2",
   "metadata": {},
   "source": [
    "# Demo: `DAOD_PHYSLITE` analysis with uproot/awkward on jupyterhub on GCP\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Note: This tutorial is targeted at users interested in R&D and technical details. Much of this is still in early development/prototyping.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476ab60-f01e-4a0a-8ec6-6eb35c756357",
   "metadata": {},
   "source": [
    "## Read and process PHYSLITE using uproot/awkward\n",
    "\n",
    "First, let's start with some general notes on reading `DAOD_PHYSLITE`\n",
    "\n",
    "The PHYSLITE ROOT files currently follow a similar structure as regular ATLAS xAODs\n",
    "\n",
    "They containing several trees, where the one holding the actual data is called `CollectionTree`. The others contain various forms of Metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145cd4e-63a4-4145-a993-d424021c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46341d-40e5-43de-8a13-cceef30ca38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = uproot.open(\"data/DAOD_PHYSLITE_21.2.108.0.art.pool.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e084a-42b4-45ba-99ad-411f4abc1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4cbbb4-6933-4c94-ae87-8a6ebcdbf561",
   "metadata": {},
   "source": [
    "### 1-D vectors\n",
    "* All branches are stored with the **highest split level**\n",
    "* In most cases data stored in branches called `Aux.<something>` or `AuxDyn.<something>`\n",
    "* Typically **vectors of fundamental types**, like e.g. pt/eta/phi of particle collections\n",
    "* **can be read into numpy arrays efficiently using uproot** since data stored as contiguous blocks  \n",
    "(except for the 10-byte vector headers whoose positions are known from ROOT's event offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264d7f5-1b56-44a2-a049-2f19f1d6f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"CollectionTree\"].show(\"/AnalysisElectronsAuxDyn.(pt|eta|phi)$/i\", name_width=30, interpretation_width=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2d506-2475-4e7d-977c-34e5b9d08fbb",
   "metadata": {},
   "source": [
    "### ElementLinks\n",
    "\n",
    "The most relevant exception to this: `ElementLink` branches:\n",
    "\n",
    "* provide cross references into other collections\n",
    "* **often 2-dimensional** (`vector<vector<ElementLink<...>>>`)\n",
    "* data part (`ElementLink`) is serialized as a **structure of 2 32bit unsigned integers**:\n",
    "  * hash `m_persKey`, identifying the target collection\n",
    "  * index `m_persIndex` identifying the array-index of the corresponding particle in the target collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9dd67-6f45-4726-9009-9fa5e8f2d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"].typename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7c2dd-ffaf-4138-ba56-94185f7c4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in f.file.streamer_named(\"ElementLinkBase\").elements:\n",
    "    print(f\"{element.member('fName')}: {element.member('fTypeName')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c86472-36bd-4455-8244-cca9f33403b0",
   "metadata": {},
   "source": [
    "Uproot can read this, but the loop that deserializes the data is done in python and therefore slow.\n",
    "\n",
    "This is not relevant for this very small file, but becomes important for larger files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183d0b7-346f-4799-af89-1ce1a56ffe05",
   "metadata": {},
   "source": [
    "This can be handled by [AwkwardForth](https://doi.org/10.1051/epjconf/202125103002) which is however currently (November 2021) not yet integrated with uproot.\n",
    "\n",
    "For now we can use a custom function `branch_to_array` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb2a87-249d-4ec5-a7fc-850817cf0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.deserialization_hacks import branch_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ad108-1d1b-4cc9-bd19-171c29db3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be240c05-fa94-4a81-8faf-02472d3bb44b",
   "metadata": {},
   "source": [
    "One can actually see a significant improvement already for the small file with only 40 events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb1766-5acd-4176-b199-4b42d63aadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using standard uproot\n",
    "f.file.array_cache.clear()\n",
    "f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718bee3-0b34-44d4-afbe-48ae22c98b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using numba\n",
    "f.file.array_cache.clear()\n",
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a462d-3026-4879-a0c1-891b8a1a36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# using awkward forth\n",
    "f.file.array_cache.clear()\n",
    "branch_to_array(f[\"CollectionTree/AnalysisElectronsAuxDyn.trackParticleLinks\"], use_forth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ba774-ad57-40d0-b39d-12c51a024461",
   "metadata": {},
   "source": [
    "## Integration with `coffea.nanoevents`\n",
    "\n",
    "The PHYSLITE schema and the corresponding behavior classes are still under development - [CoffeaTeam/coffea#540](https://github.com/CoffeaTeam/coffea/issues/540) tracks the progress of some TODO items.\n",
    "\n",
    "For more information on `NanoEvents` see the [NanoEvents tutorial](https://github.com/CoffeaTeam/coffea/blob/master/binder/nanoevents.ipynb) or [Nick Smith's presentation](https://youtu.be/udzkE6t4Mck) at the [pyHEP 2020](https://indico.cern.ch/event/882824).\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>The Goal:</b>\n",
    "    <ul>\n",
    "        <li>Work with object-oriented event data models, but stick to the array-at-a-time processing paradigm.<br> â†’ Struct/Object of arrays instead of Array of structs/objects</li>\n",
    "        <li>Hide the details from the user</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124644bb-ac41-43c2-9a5f-fb9a0088d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "\n",
    "# patch nanoevents to use the custom branch_to_array function\n",
    "from physlite_experiments.deserialization_hacks import patch_nanoevents\n",
    "patch_nanoevents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa024f-1b43-473d-847a-6982854d847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = NanoEventsFactory.from_root(\n",
    "    \"data/DAOD_PHYSLITE_21.2.108.0.art.pool.root\",\n",
    "    \"CollectionTree\",\n",
    "    schemaclass=PHYSLITESchema\n",
    ")\n",
    "events = factory.events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711d5ea-82b9-4801-bcba-f83a51c0e9e7",
   "metadata": {},
   "source": [
    "This groups particles and the available properties conveniently under one central `event` array\n",
    "\n",
    "* everything is lazy loading\n",
    "* cross referencing via ElementLinks already implemented for some collections\n",
    "* particles behave as LorentzVectors (can add them, calculate invariant masses and much more)\n",
    "\n",
    "See [my tutorial at the IRIS-HEP AGC tools workshop 2021](https://github.com/nikoladze/agc-tools-workshop-2021-physlite) for more technical details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b42c6-7dd5-4ef8-9027-8e9aecbd6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cdf33-0d49-43f7-a244-1376884150f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aebe36-ab05-41df-87b6-f4d78f44be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dbf93-e788-43af-89f4-fea097e3194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.trackParticles.z0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698fb19-d255-459a-85ba-5eb7dd8a88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons[events.Electrons.pt > 10000].trackParticles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a6ce0-d89b-4253-ba11-a6a979ad941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f96869-ac2e-4c16-b3a0-04efef9ac73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42739469-b1ff-4842-aaea-39f163416c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8410153-cd43-4e60-9728-f6576dc2f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children.parents.children.pdgId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be845283-8086-4867-9d62-cea7886b04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "events.TruthElectrons.parents.children.parents.children.pdgId.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0fc00-b3d1-453f-8263-6dde5184587c",
   "metadata": {},
   "source": [
    "## Read data via HTTPS from google cloud storage (authentication via rucio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a862289-18cd-441b-a44b-c6fe7ffc6fe7",
   "metadata": {},
   "source": [
    "*Now we are going to do something a bit weird: instead of importing some utility functions we will directly execute a python file containing them. This is because we later want dask to serialize the functions to send them to the workers (which don't have access to our local directory on the submission node). It's a workaround for interactively developing functions that are sent to dask workers on a dask gateway cluster (which is used here). This issue does not occur in a setting where you have a shared filesystem for all workers.*\n",
    "\n",
    "**Let me know if you know a better approach - one alternative is dask's `upload_file` method, but that has it's own issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f8229-3053-451c-bbea-c1c8db34271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68529d09-6eb2-4fdf-a23d-152ff45e11f3",
   "metadata": {},
   "source": [
    "this gives us the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ab68f-af36-4e73-a43b-cd5b9ee8fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_rucio_and_proxy, get_signed_url, get_signed_url_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f8f03-276b-4583-8d02-db7263681a6a",
   "metadata": {},
   "source": [
    "We will use them to authenticate to rucio and get signed urls on google cloud storage (GCS).\n",
    "\n",
    "For that we have to provide a VOMS proxy. To avoid the need for having the grid certificate and the voms tools on this jupyterhub instance we create the voms proxy outside (some machine where we have the voms tools and our grid certificate) and upload it to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b42925-4ea8-4a7f-b3a5-d96e58d9c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload\n",
    "upload = FileUpload()\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624494a-16d1-4d6f-9f4e-136aceaeca66",
   "metadata": {},
   "source": [
    "Then we setup the nescessary environment variables (fill in your cern account name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaecc2d-774c-48b4-a36b-d1a2a6e12844",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_rucio_and_proxy(upload.data[-1], rucio_account=\"nihartma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e38f1-88f4-4cc5-9994-0b87212ad6b4",
   "metadata": {},
   "source": [
    "Now we should be able to query rucio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbe046-c3e4-4f6f-bc20-b880ef5ca7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rucio.client\n",
    "rucio_client = rucio.client.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9e554-0bda-40e3-beab-9387bb53905d",
   "metadata": {},
   "source": [
    "Let's get a list of all files in one data period, corresponding to around 10% of the whole Run2 data - around 10TB in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c56a1d-ccaf-4f12-af91-a50aadbc34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(rucio_client.list_files(\"data17_13TeV\", \"data17_13TeV.periodK.physics_Main.PhysCont.DAOD_PHYSLITE.grp17_v01_p4309\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b637740-e1bf-4fca-b115-8b35c5aab437",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b178-fc6c-465c-878a-9aed678d4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(file[\"bytes\"] for file in files) / 1024 ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a75ef1-22b0-4469-a530-61374cee93eb",
   "metadata": {},
   "source": [
    "The full Run2 dataset is replicated to GCS. To access it via https we can ask rucio for a signed url. Uproot can directly deal with http(s) urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353ee01-b541-49f3-98b3-69ce57542e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = get_signed_url(rucio_client, files[0][\"scope\"], files[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ade9f-7030-43c7-b3dc-6c9c345cbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_remote = uproot.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc588b-e751-485d-975f-e1456a041017",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_remote[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6b9a4-a030-4359-abee-25a77ad5cadd",
   "metadata": {},
   "source": [
    "Some notes on this:\n",
    "\n",
    "* GCS does not support multi-range requests (equivalent to xrootd vector reads), single-range requests are allowed\n",
    "* Single-range requests with the uproot `MultithreadedHTTPSource` are suboptimal\n",
    "* GCS seems fine with a huge number of parallel requests - this can be done with asyncio\n",
    "* However, oftentimes downloading the whole file is still faster async reading of partial chunks (but needs lot's of memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27d263-4406-48bc-a717-0b7114e9cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(url):\n",
    "    return requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617004cb-6b0b-43ce-9959-71d314094512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b98122-3bff-4eb5-843e-71d07ec26354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "uproot.open(io.BytesIO(data))[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5279025-93c9-4278-9326-d704122d9ba6",
   "metadata": {},
   "source": [
    "I have an experimental implementation for an asyncio HTTPSource for uproot (should probably make a PR for uproot at some point or consider using an interface to fsspec which has a `cat_ranges` method that might be used for this).\n",
    "\n",
    "GCS seems fine with 100 parallel tcp connections (even for each worker on a larger cluster):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53b818-357a-4ffc-9ec5-816011776b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physlite_experiments.io import AIOHTTPSource\n",
    "\n",
    "class AIOHTTP100Source(AIOHTTPSource):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, tcp_connection_limit=100, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690236d-ff68-4fea-82bd-621e8e5c4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uproot.open(url, http_handler=AIOHTTP100Source)[\"CollectionTree/AnalysisElectronsAuxDyn.pt\"].array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77635c1e-613e-454d-95ab-8bb93aa63d0b",
   "metadata": {},
   "source": [
    "## Run an actual analysis with this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc40a49-473c-407e-8b57-e5d3eff4341c",
   "metadata": {},
   "source": [
    "## Run on a dask cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
